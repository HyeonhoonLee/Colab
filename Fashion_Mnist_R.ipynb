{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion_Mnist_CResNet의+cutout",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeonhoonLee/Colab/blob/master/Fashion_Mnist_R.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MhoQ0WE77laV"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "_ckMIh7O7s6D",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "vasWnqRgy1H4",
        "colab": {}
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# 첫 번째 신경망 훈련하기: 기초적인 분류 문제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />TensorFlow.org에서 보기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩(Colab)에서 실행하기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />깃허브(GitHub) 소스 보기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/keras/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aJjNjjy3Zbz0"
      },
      "source": [
        "Note: 이 문서는 텐서플로 커뮤니티에서 번역했습니다. 커뮤니티 번역 활동의 특성상 정확한 번역과 최신 내용을 반영하기 위해 노력함에도\n",
        "불구하고 [공식 영문 문서](https://www.tensorflow.org/?hl=en)의 내용과 일치하지 않을 수 있습니다.\n",
        "이 번역에 개선할 부분이 있다면\n",
        "[tensorflow/docs-l10n](https://github.com/tensorflow/docs-l10n/) 깃헙 저장소로 풀 리퀘스트를 보내주시기 바랍니다.\n",
        "문서 번역이나 리뷰에 참여하려면\n",
        "[docs-ko@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-ko)로\n",
        "메일을 보내주시기 바랍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FbVhjPpzn6BM"
      },
      "source": [
        "이 튜토리얼에서는 운동화나 셔츠 같은 옷 이미지를 분류하는 신경망 모델을 훈련합니다. 상세 내용을 모두 이해하지 못해도 괜찮습니다. 여기서는 완전한 텐서플로(TensorFlow) 프로그램을 빠르게 살펴 보겠습니다. 자세한 내용은 앞으로 배우면서 더 설명합니다.\n",
        "\n",
        "여기에서는 텐서플로 모델을 만들고 훈련할 수 있는 고수준 API인 [tf.keras](https://www.tensorflow.org/guide/keras)를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dzLKpmZICaWN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2c880f5-7422-4f7e-f439-f8c5c660c6dd"
      },
      "source": [
        "# tensorflow와 tf.keras를 임포트합니다\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# 헬퍼(helper) 라이브러리를 임포트합니다\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## 패션 MNIST 데이터셋 임포트하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DLdCchMdCaWQ"
      },
      "source": [
        "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋을 사용하겠습니다. 이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 옷 품목을 나타냅니다:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>그림 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "패션 MNIST는 컴퓨터 비전 분야의 \"Hello, World\" 프로그램격인 고전 [MNIST](http://yann.lecun.com/exdb/mnist/) 데이터셋을 대신해서 자주 사용됩니다. MNIST 데이터셋은 손글씨 숫자(0, 1, 2 등)의 이미지로 이루어져 있습니다. 여기서 사용하려는 옷 이미지와 동일한 포맷입니다.\n",
        "\n",
        "패션 MNIST는 일반적인 MNIST 보다 조금 더 어려운 문제이고 다양한 예제를 만들기 위해 선택했습니다. 두 데이터셋은 비교적 작기 때문에 알고리즘의 작동 여부를 확인하기 위해 사용되곤 합니다. 코드를 테스트하고 디버깅하는 용도로 좋습니다.\n",
        "\n",
        "네트워크를 훈련하는데 60,000개의 이미지를 사용합니다. 그다음 네트워크가 얼마나 정확하게 이미지를 분류하는지 10,000개의 이미지로 평가하겠습니다. 패션 MNIST 데이터셋은 텐서플로에서 바로 임포트하여 적재할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7MqDQO0KCaWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "58586416-27f7-4797-b8f5-ddc98a679c3b"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9FDsUlxCaWW"
      },
      "source": [
        "load_data() 함수를 호출하면 네 개의 넘파이(NumPy) 배열이 반환됩니다:\n",
        "\n",
        "* `train_images`와 `train_labels` 배열은 모델 학습에 사용되는 *훈련 세트*입니다.\n",
        "* `test_images`와 `test_labels` 배열은 모델 테스트에 사용되는 *테스트 세트*입니다.\n",
        "\n",
        "이미지는 28x28 크기의 넘파이 배열이고 픽셀 값은 0과 255 사이입니다. *레이블*(label)은 0에서 9까지의 정수 배열입니다. 이 값은 이미지에 있는 옷의 *클래스*(class)를 나타냅니다:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>레이블</th>\n",
        "    <th>클래스</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "각 이미지는 하나의 레이블에 매핑되어 있습니다. 데이터셋에 *클래스 이름*이 들어있지 않기 때문에 나중에 이미지를 출력할 때 사용하기 위해 별도의 변수를 만들어 저장합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IjnLH5S2CaWx",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Brm0b_KACaWX"
      },
      "source": [
        "## 데이터 탐색\n",
        "\n",
        "모델을 훈련하기 전에 데이터셋 구조를 살펴보죠. 다음 코드는 훈련 세트에 60,000개의 이미지가 있다는 것을 보여줍니다. 각 이미지는 28x28 픽셀로 표현됩니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zW5k_xz1CaWX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8369483-c11c-4b4e-e392-7d773d8b3d62"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drITXA8fMtDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "fb11cf6f-ed95-4c16-ba7c-b1303d819320"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAD4CAYAAACE9dGgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAc7ElEQVR4nO3de3Bc5Znn8e8jWfJFlm/YCANODMQkcZLFsA4QoDIkzIRLpcawyVBQs8SZocbsLuyEKf6AYWcrbE2xRWUDbGYyYccENqYKwjIBFoZxhYtDQkiGizEOvi2xARNjfDfYxrZsqfvZP/ootCyd5xypW+o+5vehTql1nn77vD6SHs7lOe9r7o6ISFG1NLoDIiK1UBITkUJTEhORQlMSE5FCUxITkUIbM5oba7exPo6O0dykyEdKN/s57Iesls+48Esdvmt3Kdd7X3nt0JPuflEt26tVTUnMzC4Cvge0Aj9099ui94+jg7Psglo2KSKBF31ZzZ+xa3eJl578WK73ts5cP73mDdZo2KeTZtYK/ANwMTAXuNLM5tarYyLSGA6Uc/6XxcxmmdmzZrbWzNaY2beS9beY2WYzW5ksl1S1+Wsz22Bmr5vZhVnbqOVI7Exgg7u/mWz4QWABsLaGzxSRBnOcHs93OplDL3CDu68ws07gFTN7Oond6e7frX5zciB0BfAZ4HjgGTM71T29Q7Vc2D8B2FT1/TvJun7MbJGZLTez5T0cqmFzIjJa6nUk5u5b3H1F8nofsI5B8kSVBcCD7n7I3d8CNlA5YEo14ncn3X2xu8939/ltjB3pzYlIjRyn5PkWYHrfQUqyLEr7XDObDZwOvJisus7MXjOze81sarIu18FRtVqS2GZgVtX3JybrRKTgyniuBdjZd5CSLIsH+zwzmwg8DFzv7nuBu4BTgHnAFuD24fa1liT2MjDHzE4ys3Yq57GP1/B5ItIEHCjhuZY8zKyNSgK7390fAXD3be5ecvcycDcfnjIO+eBo2EnM3XuB64AnqZznPuTua4b7eSLSPIZwJBYyMwPuAda5+x1V62dWve0yYHXy+nHgCjMba2YnAXOAl6Jt1FQn5u5LgaW1fIaINBcHeuo3RNe5wFXAKjNbmay7mUpJ1rxkcxuBawDcfY2ZPUSlyqEXuDa6MwmjXLEvIs3Ph3CqmPlZ7s8Dgz1BkHrw4+63Arfm3YaSmIj051Aq0FipSmIi0k+lYr84lMRE5AhGadAzwOakJCYi/VQu7CuJiUhBVerElMREpMDKOhITkaLSkZiIFJpjlAo0cr2SmIgMoNNJESksxzjsrY3uRm5KYiLST6XYVaeTIlJgurAvzcMyfhlrHK2g9ZhpYfy9C09NjU164IWatp31b7Mxbakx7zlc27ZrlfVzidRvhImUjzdKriMxESmwso7ERKSoKhf2i5MaitNTERkVurAvIoVXUp2YiBSVKvZFpPDKujspIkVVeQBcSUyahLXGj494b28Yb5k3N4yvu2Zi3P5geqxtfzg7PWMOxoMktz21PIzXVAuWVYOWsV+xOAnU0jcbE/zZxj/OXByjR48diUhRuaNiVxEpMlOxq4gUl6MjMREpOF3YF5HCckyDIopIcVWmbCtOaihOT0VklGjyXGkiYU0R2XVimy6cEsb/9Au/DOO/2nFyauztsceFbX18GGbMH34hjJ/6g82psd6Nv4s/PGPMrqz9lqV16tT0YKkUti3t3ZserMNQY85HqGLfzDYC+4AS0Ovu8+vRKRFprI/akdiX3H1nHT5HRJqAu310jsRE5OhTubD/0XnsyIGnzMyBf3T3xUe+wcwWAYsAxjGhxs2JyMgr1hj7tfb0PHc/A7gYuNbMvnjkG9x9sbvPd/f5bYytcXMiMtIqF/Yt15LFzGaZ2bNmttbM1pjZt5L108zsaTNbn3ydmqw3M/s7M9tgZq+Z2RlZ26gpibn75uTrduBRIB6WQEQKoURLriWHXuAGd58LnE3lYGcucBOwzN3nAMuS76FyQDQnWRYBd2VtYNhJzMw6zKyz7zXwFWD1cD9PRJpDX8V+PY7E3H2Lu69IXu8D1gEnAAuAJcnblgCXJq8XAPd5xQvAFDObGW2jlmtiXcCjVhl3aQzwgLv/tIbPkxFQ7u6uqf3h0z8I41+fHI/pNa6lJzX2i5Z4vLDNP5sVxkv/Ju7b23d0psbKr54Ttj1mdVyrNenVLWF85xdPCOM7/m16QVdXxnScU595IzVmu+tzr24IE4VMN7PqX4LFg10bBzCz2cDpwItAl7v37cStVPIJVBLcpqpm7yTrUnf4sP/F7v4mcNpw24tIc3KHnnLuJLYzT32omU0EHgaud/e9VjXopLt7cnNwWFRiISL9VE4n63d30szaqCSw+939kWT1NjOb6e5bktPF7cn6zUD1IfiJybpUxbmPKiKjppQ8P5m1ZLHKIdc9wDp3v6Mq9DiwMHm9EHisav03kruUZwN7qk47B6UjMRHpp6/Eok7OBa4CVpnZymTdzcBtwENmdjXwNnB5ElsKXAJsAA4Af5a1ASUxETlC/U4n3f15SD1ku2CQ9ztw7VC2oSQmIgNojH0ZXdH0YhlDynxw+dlh/Btzfx7G3+iZEcZPbN+dGvuT418J2/Lv4/j3X/+DML7/zcmpsZaOeL9sPTs+Etm8IP53e088VM/UFel/ei0Lt4Vt9x5OH96otKz2p2Iqdyc/Os9OishRRsNTi0jh6XRSRAqrzncnR5ySmIgMoEERRaSw3I1eJTERKTKdTopIYemamAxdVOc1ws6+8aUw/qWJa2v6/BOCOcT2e3vY9v1SRxj/9tx/CeM7Tk0fiidrctgfro+H6vkgqEEDaO2Nf6Zn//mrqbGvTXs5bPudhz+XGmvx/WHbvJTERKSwVCcmIoWnOjERKSx36M0/KGLDKYmJyAA6nRSRwtI1MREpPFcSE5Ei04V9GZqMMb9G0voPjg3juyZNDONbe6eE8WNa06dV62w5GLad3bYzjO8opdeBAbS2pU8Jd9jj8bL+22f+OYx3f7otjLdZPOXbOePeTY39ydpvhG07eDOM18pd18REpNCMku5OikiR6ZqYiBSWnp0UkWLzhl6mHTIlMREZQHcnRaSwXBf2RaTodDophTFjbHodF8A46wnj7RbPr/huz9TU2PqDnwzb/nZvXMN2UdeaMN4T1IK1BuOcQXad1/Ft74Xxbo/ryKK9em5XXAe2MozWR5HuTmYeM5rZvWa23cxWV62bZmZPm9n65Gv6b6qIFIp7JYnlWZpBnhPfHwEXHbHuJmCZu88BliXfi8hRouyWa2kGmUnM3Z8DjpyLfgGwJHm9BLi0zv0SkQZyz7c0g+FeE+ty9y3J661AV9obzWwRsAhgHBOGuTkRGS2OUS7Q3cmae+ruDulXSd19sbvPd/f5bYytdXMiMgo859IMhpvEtpnZTIDk6/b6dUlEGuoovLA/mMeBhcnrhcBj9emOiDSFAh2KZV4TM7MfA+cD083sHeDbwG3AQ2Z2NfA2cPlIdvKolzHvpLXGY195b3qtVuvUuPrlD6asCuM7SpPC+Pul+DrnlNYDqbF9vePCtrsPxp/9qbFbwviKA7NTYzPa4zqvqN8AGw9PD+Nzxm4N49/ZdkFqbNa4I++j9dd7wRdTY/7iv4Zt82qWo6w8MpOYu1+ZEkr/KYhIYTlQLtcniZnZvcBXge3u/tlk3S3AXwA7krfd7O5Lk9hfA1cDJeAv3f3JrG0U5xaEiIwOB9zyLdl+xMA6U4A73X1esvQlsLnAFcBnkjY/MLP4NAQlMREZRL3qxFLqTNMsAB5090Pu/hawATgzq5GSmIgMlP/C/nQzW161LMq5hevM7LXksca+C7cnAJuq3vNOsi6kB8BF5AhDKp/Y6e7zh7iBu4C/pZIG/xa4HfjzIX7G7+lITEQGGsESC3ff5u4ldy8Dd/PhKeNmYFbVW09M1oV0JNYMMi4u2Jj4xxSVWGy6+tNh2y9PiKcm+3V3fDQ/Y8y+MB4NhzNz7J6wbWdXdxjPKu+YNiZ9mKF9pfFh2wkth8J41r/7jPZ4urm/euaM1FjnZ3eFbSe1Bcce9bip6OB1ujs5GDObWfXY4mVA3wg5jwMPmNkdwPHAHOClrM9TEhORQdStxGKwOtPzzWwelWO5jcA1AO6+xsweAtYCvcC17h4P7IaSmIgMpk7V+Cl1pvcE778VuHUo21ASE5GBmuSRojyUxESkv75i14JQEhORAZplwMM8lMREZKARvDtZb0piIjKA6UhMhsLa2sN4uTuul4pMX3U4jO8sxVOLTWmJh6Rpz5ja7HBQJ3bOtLfCtjsyarlWHDwpjHe2HkyNzWiJ67xmtcW1Wqu6Z4Xxpfs/Ecav/uozqbEfL/6jsG37T3+dGjOPf165NNFYYXkoiYnIEXKPUNEUlMREZCAdiYlIoZUb3YH8lMREpD/ViYlI0enupIgUW4GSmMYTE5FCK9aRWDC1mY2J652sNSNft8TxcncwvlQ5c7SQkPfEtVy1+N4/fj+Mb+qdEsa39sTxrKnNSsGQLi8cnBy2HdfSE8ZnjNkbxveW4zqzyL5yPJ1cNE4aZPf9xmPWp8Ye2fOHYdvRoNNJESkuR48diUjB6UhMRIpMp5MiUmxKYiJSaEpiIlJU5jqdFJGi093J4allfsWsWiuPy3Ya6uCCM8P4pkvjOrQ/PT19ar6tvZ1h21cPzA7jk4MxuQA6MuZn7Pb0+r13D09NjUF2rVU0ryTAsUEdWcnjusDNPXHfsmTVz73TG8yJ+cfxWGdT7htWl4akSEdimRX7ZnavmW03s9VV624xs81mtjJZLhnZborIqBrBGcDrLc9jRz8CLhpk/Z3uPi9Zlta3WyLSMP7hdbGspRlkJjF3fw7YPQp9EZFmcZQdiaW5zsxeS043Uy8gmNkiM1tuZst7iK+fiEhzsHK+pRkMN4ndBZwCzAO2ALenvdHdF7v7fHef38bYYW5ORGRww0pi7r7N3UvuXgbuBuLbayJSLEf76aSZzaz69jJgddp7RaRgCnZhP7NOzMx+DJwPTDezd4BvA+eb2TwquXgjcE09OhPVgdVqzMzjwnjPSV1hfPenJ6TGDhwXFwbOu2RdGP9m1/8O4ztKk8J4m6Xvt009x4RtT5+wMYz/bM/cML5zzMQwHtWZndORPqYWwPvl9H0OcPyY98L4jRu+nhrrmhDXYv3w4/EN9x6PLwi93hNfOtlTTh+P7C/nPhu2fZQZYbwumiRB5ZGZxNz9ykFW3zMCfRGRZnE0JTER+WgxmufOYx5KYiLSXxNd78pDE4WIyEB1ujuZ8tjiNDN72szWJ1+nJuvNzP7OzDYkNahn5OmqkpiIDFS/EosfMfCxxZuAZe4+B1iWfA9wMTAnWRZRqUfNpCQmIgPUq8Qi5bHFBcCS5PUS4NKq9fd5xQvAlCPKuQbVVNfEDl38+TB+7H95MzU2b9I7Ydu5458P493leMq3aFiYtQdPCNseKLeH8fWH4/KPPb1xqUFrcBV2++F4KJ7b34qnB1t25v8K43/z7mBjA3yoZXz6b/quUlye8bWJ8ZRsEP/MrvnYc6mxk9u3h22f2B//7bybMVRPV9ueMD67bUdq7N91/jZsexSUWHS5+5bk9Vagr77pBGBT1fveSdZtIdBUSUxEmoAP6e7kdDNbXvX9YndfnHtT7m5W220EJTERGSh/Wtnp7vOH+OnbzGymu29JThf7Dos3A7Oq3ndisi6ka2IiMsAIP3b0OLAweb0QeKxq/TeSu5RnA3uqTjtT6UhMRAaq0zWxlMcWbwMeMrOrgbeBy5O3LwUuATYAB4A/y7MNJTER6a+OI1SkPLYIcMEg73Xg2qFuQ0lMRPoxilWxryQmIgMoiaWxeFq2s/77y2HzCzrXpMYOeDz0SVYdWFbdT2TymHh6rkM98W7e3hMPtZPl1LFbU2OXTVoZtn3u+2eF8fO6/3MYf+PL8TBCyw6mDzmzozf+d1/x1pfD+IrfzQrjZ89+KzX2uc74pldWbV5na3cYj4ZHAthfTv99faE7rp8bFUpiIlJoSmIiUlgFG8VCSUxEBlISE5Ei06CIIlJoOp0UkeJqounY8lASE5GBlMQG13NsB+9elT7P7i2T/z5s/8Dus1Njs8YdOe5afx9v3xnGTxv/dhiPdLbENUOfnBTXDD2x/8Qw/vP3PxXGZ7a9nxr75YFTwrYP3vI/wvg3/+qGMP6Fpf8hjO+dnT7GQG9H/Jcy6bRdYfxvTv+XMN5updTY+6W4Dmza2P1hfEprXBuYJapr7GxJn+YOoPWTn0iN2cZ43Lw8VLEvIoVn5eJkMSUxEelP18REpOh0OikixaYkJiJFpiMxESk2JTERKayhzXbUcKOaxFp6YMK29L3zxN55YfuTx6fP1bezJ55f8ckPPhfGTxz/Xhif3Jpeu/OJYDwvgJXdU8L4T3d8JowfPz6ef3Fbz+TU2K6ejrDtgWBcK4B77rwjjN++LZ638rJpK1Jjp7XHdWDvl+N5bNZmzNe5rzwuNdbt8fhyezLqyDqD3weAHo//tFo9/e9gSktcg7b3c8ekxkrbav+TLlqdWOZsR2Y2y8yeNbO1ZrbGzL6VrJ9mZk+b2frk6/BHFRSR5uKeb2kCeaZs6wVucPe5wNnAtWY2F7gJWObuc4BlyfcichQY4Snb6iozibn7FndfkbzeB6yjMrX4AmBJ8rYlwKUj1UkRGUU+hKUJDOkE2sxmA6cDLwJdVRNbbgW6UtosAhYBtHfojFOkCIp0YT/3DOBmNhF4GLje3ftdaU7mixs0L7v7Ynef7+7zx4yNLzKLSHOwcr6lGeRKYmbWRiWB3e/ujySrt5nZzCQ+E9g+Ml0UkVHlFOrCfubppJkZcA+wzt2r77c/DiykMiX5QuCxrM9qPVymc9Oh1HjZLWz/s53pQ9J0jdsXtp3XuSmMv34gvl2/6uDxqbEVYz4Wth3f2hPGJ7fHQ/l0jEnfZwDT29L/7SeNjf/fEg1XA/Byd/xv+48zfh7Gf9ebfgnhn/efGrZdeyB9nwNMzZgqb9Xe9PYHetvDtodK8Z9Gd29csjN5bPwz/fy09KGfXmdm2HbHacHwRr8Km+bWLBft88hzTexc4CpglZn1TWJ4M5Xk9ZCZXQ28DVw+Ml0UkVF3NCUxd3+eSv3bYC6ob3dEpNGKVuyqx45EpD93DYooIgVXnBymJCYiA+l0UkSKywGdTopIoRUnh41yEvvgIC2/eDU1/E9PnRs2/68L/ik19ouMac2e2BrX9ew9HA9JM2NC+hRek4I6LYBpbfH0X5Mz6p3GWTzl23u96U9CHGqJh5wppd54rth6KH2YH4BfleeE8Z5ya2rsUBCD7Pq63Yenh/Hjx+9Jje3rTR+mB2DjvmlhfOeeiWG8e0L8p/V8KX0qvYuOWxO2Hb89/WfWEv+q5KbTSREptHrenTSzjcA+oAT0uvt8M5sG/B9gNrARuNzd40H9UuR+dlJEPiJGZhSLL7n7PHefn3xft6G8lMREpJ9KsavnWmpQt6G8lMREZKByzgWmm9nyqmXRIJ/mwFNm9kpVPNdQXnnompiIDDCEo6ydVaeIac5z981mdizwtJn9v+qgu7vZ8G8l6EhMRPqr8zUxd9+cfN0OPAqcSR2H8lISE5EjVJ6dzLNkMbMOM+vsew18BVjNh0N5Qc6hvNI01enkyTf+axj/wWtfT2/7n14P21583OowvmJvPG7W74K6od8EY40BtLXEQ2BOaDscxsdl1Eu1t6aPCdaS8b/LckadWEdr3Lessc6mjU2vketsjcfcaqlx6NDW4N/+0p7ZYduuCXHt3ycm7QzjvR4fH3xh8hupsXvfOids2/X3v06NbfS4JjG3+g142AU8WhmWkDHAA+7+UzN7mToN5dVUSUxEmkAdJ8919zeB0wZZv4s6DeWlJCYiAzXJ0NN5KImJyEDFyWFKYiIykJWbZCqjHJTERKQ/p6+QtRCUxESkH6PmR4pGlZKYiAykJBZoCcaQKsdzIE6+/4XU2K77483+5GsXhvGzbn45jH919m9SY59q3xa2bcs4Nh+XcT+7oyWu5eoOfuGyqpmfPzgrjJcyPuFn7306jL/fMz41tu3ApLBtW1D/lkc0j+nB3nictT0H4/HGWlviP/Lun8djnb21Nn38u8lL49/FUaEkJiKFpWtiIlJ0ujspIgXmOp0UkQJzlMREpOCKczapJCYiA6lOTESK7WhKYmY2C7iPyrhADix29++Z2S3AXwA7krfe7O5LM7eYUQs2UjoefjGMr344br+ak1Jj9vk/DtsePC69Vgpg7K54TK59H4/bT3ojfQyplkPxRITl36wL49k+qKHt3jAaj6JWm/aM+Iyat/Dbmj+hYdyhVJzzyTxHYr3ADe6+Ihmh8RUzezqJ3enu3x257olIQxxNR2LJjCRbktf7zGwdcMJId0xEGqhASWxIY+yb2WzgdKDv3Ow6M3vNzO41s6kpbRb1TefUQ3zaJCJNwIGy51uaQO4kZmYTgYeB6919L3AXcAowj8qR2u2DtXP3xe4+393ntzG2Dl0WkZHl4OV8SxPIdXfSzNqoJLD73f0RAHffVhW/G3hiRHooIqPLKdSF/cwjMatMU3IPsM7d76haP7PqbZdRmYZJRI4G7vmWJpDnSOxc4CpglZmtTNbdDFxpZvOo5O2NwDUj0sMC8JdXhfF4UJdsk9Jn6MpUnP+fSlNpkgSVR567k8/DoJMTZteEiUgBNc9RVh6q2BeR/hzQUDwiUmg6EhOR4jr6HjsSkY8SB2+SGrA8lMREZKAmqcbPQ0lMRAbSNTERKSx33Z0UkYLTkZiIFJfjpcYMXjocSmIi0l/fUDwFoSQmIgMVqMRiSIMiisjRzwEve64lDzO7yMxeN7MNZnZTvfurJCYi/Xn9BkU0s1bgH4CLgblURr+ZW8/u6nRSRAao44X9M4EN7v4mgJk9CCwA1tZrA6OaxPbx3s5n/CdvV62aDuwczT4MQbP2rVn7BerbcNWzbx+v9QP28d6Tz/hPpud8+zgzW171/WJ3X1z1/QnApqrv3wHOqrWP1UY1ibl7v+n8zGy5u88fzT7k1ax9a9Z+gfo2XM3WN3e/qNF9GApdExORkbQZmFX1/YnJurpREhORkfQyMMfMTjKzduAK4PF6bqDRF/YXZ7+lYZq1b83aL1DfhquZ+1YTd+81s+uAJ4FW4F53X1PPbZgX6BkpEZEj6XRSRApNSUxECq0hSWykH0OohZltNLNVZrbyiPqXRvTlXjPbbmarq9ZNM7OnzWx98nVqE/XtFjPbnOy7lWZ2SYP6NsvMnjWztWa2xsy+laxv6L4L+tUU+62oRv2aWPIYwm+BP6JS+PYycKW7162CtxZmthGY7+4NL4w0sy8CHwD3uftnk3XfAXa7+23J/wCmuvuNTdK3W4AP3P27o92fI/o2E5jp7ivMrBN4BbgU+CYN3HdBvy6nCfZbUTXiSOz3jyG4+2Gg7zEEOYK7PwfsPmL1AmBJ8noJlT+CUZfSt6bg7lvcfUXyeh+wjkrleEP3XdAvqUEjkthgjyE00w/SgafM7BUzW9Tozgyiy923JK+3Al2N7MwgrjOz15LTzYac6lYzs9nA6cCLNNG+O6Jf0GT7rUh0YX+g89z9DCpP3V+bnDY1Ja9cC2imGpm7gFOAecAW4PZGdsbMJgIPA9e7+97qWCP33SD9aqr9VjSNSGIj/hhCLdx9c/J1O/AoldPfZrItubbSd41le4P783vuvs3dS16ZtPBuGrjvzKyNSqK4390fSVY3fN8N1q9m2m9F1IgkNuKPIQyXmXUkF1wxsw7gK8DquNWoexxYmLxeCDzWwL7005cgEpfRoH1nZgbcA6xz9zuqQg3dd2n9apb9VlQNqdhPbiH/Tz58DOHWUe/EIMzsZCpHX1B5JOuBRvbNzH4MnE9lqJZtwLeB/ws8BHwMeBu43N1H/QJ7St/Op3JK5MBG4Jqqa1Cj2bfzgF8Cq4C+kftupnL9qWH7LujXlTTBfisqPXYkIoWmC/siUmhKYiJSaEpiIlJoSmIiUmhKYiJSaEpiIlJoSmIiUmj/H4BqExLuMX2fAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJR6_clwGoja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "627c2608-13f8-485d-fce6-e4a2b9a6f7b2"
      },
      "source": [
        " # Tensor로 변환\n",
        " train_images = np.expand_dims(train_images, axis=-1)\n",
        " train_images.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cIAcvQqMCaWf"
      },
      "source": [
        "비슷하게 훈련 세트에는 60,000개의 레이블이 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TRFYHB2mCaWb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5005b1c-bfe8-4564-f959-53ed419c6c64"
      },
      "source": [
        "len(train_labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YSlYxFuRCaWk"
      },
      "source": [
        "각 레이블은 0과 9사이의 정수입니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XKnCTHz4CaWg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f8d8ac2-142f-436d-cd15-d1eb01c8a28b"
      },
      "source": [
        "train_labels"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TMPI88iZpO2T"
      },
      "source": [
        "테스트 세트에는 10,000개의 이미지가 있습니다. 이 이미지도 28x28 픽셀로 표현됩니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ELmpMl9HZSj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd4b2228-6dc0-4e34-a9d9-5b32217a5a25"
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rd0A0Iu0CaWq"
      },
      "source": [
        "테스트 세트는 10,000개의 이미지에 대한 레이블을 가지고 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3dc-dxnHG7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a032d668-574c-4d12-8896-3c6feb529811"
      },
      "source": [
        " test_images = np.expand_dims(test_images, axis=-1)\n",
        " test_images.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJmPr5-ACaWn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb034f35-624c-402e-ba82-986f1164512a"
      },
      "source": [
        "len(test_labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ES6uQoLKCaWr"
      },
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "네트워크를 훈련하기 전에 데이터를 전처리해야 합니다. 훈련 세트에 있는 첫 번째 이미지를 보면 픽셀 값의 범위가 0~255 사이라는 것을 알 수 있습니다:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wz7l27Lz9S1P"
      },
      "source": [
        "신경망 모델에 주입하기 전에 이 값의 범위를 0~1 사이로 조정하겠습니다. 이렇게 하려면 255로 나누어야 합니다. *훈련 세트*와 *테스트 세트*를 동일한 방식으로 전처리하는 것이 중요합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9kYIcZgRYep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# valiation split\n",
        "val_images = train_images[-10000:]\n",
        "val_labels = train_labels[-10000:]\n",
        "train_images = train_images[:-10000]\n",
        "train_labels = train_labels[:-10000]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8wEgyB_9KPp",
        "colab_type": "text"
      },
      "source": [
        "### Cutout regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJJXKsO49JSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cutout(images, cut_length):\n",
        "    \"\"\"\n",
        "    Perform cutout augmentation from images.\n",
        "    :param images: np.ndarray, shape: (N, H, W, C).\n",
        "    :param cut_length: int, the length of cut(box).\n",
        "    :return: np.ndarray, shape: (N, h, w, C).\n",
        "    \"\"\"\n",
        "\n",
        "    H, W, C = images.shape[1:4]\n",
        "    augmented_images = []\n",
        "    for image in images:    # image.shape: (H, W, C)\n",
        "        image_mean = int(image.mean(keepdims=True))\n",
        "        image -= image_mean\n",
        "\n",
        "        mask = np.ones((H, W, C), np.float32)\n",
        "\n",
        "        y = np.random.randint(H)\n",
        "        x = np.random.randint(W)\n",
        "        length = cut_length\n",
        "\n",
        "        y1 = np.clip(y - (length // 2), 0, H)\n",
        "        y2 = np.clip(y + (length // 2), 0, H)\n",
        "        x1 = np.clip(x - (length // 2), 0, W)\n",
        "        x2 = np.clip(x + (length // 2), 0, W)\n",
        "\n",
        "        mask[y1: y2, x1: x2] = 0.\n",
        "        image = image * mask\n",
        "\n",
        "        image += image_mean\n",
        "        augmented_images.append(image)\n",
        "\n",
        "    return np.stack(augmented_images)    # shape: (N, h, w, C)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1ualv0V-KRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ee11e2a-3985-44fd-8914-ab544e9c81a2"
      },
      "source": [
        "train_images_cut = train_images.copy()\n",
        "cutout(train_images_cut, 2)\n",
        "train_images_cut.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly7yWxx0AINs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ff8c6f8-2852-433f-9bde-cb359719adc6"
      },
      "source": [
        "# concatenate: train\n",
        "train_images = np.concatenate((train_images,train_images_cut),axis=0)\n",
        "train_images.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlZvi6HeAmLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d999201-9802-4cbd-895f-bbc2fc22bbf5"
      },
      "source": [
        "# concatenate: labels\n",
        "train_labels1=train_labels.copy()\n",
        "train_labels = np.concatenate((train_labels, train_labels1),axis=0)\n",
        "train_labels.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNkLy9sC9P5O",
        "colab_type": "text"
      },
      "source": [
        "### ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsZRl36mQlAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# train_dataset\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    featurewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=False)\n",
        "\n",
        "train_generator = train_datagen.flow(\n",
        "    train_images, train_labels,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "# val_dataset\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)  #127.5 -1..?\n",
        "\n",
        "val_generator = val_datagen.flow(\n",
        "    train_images, train_labels,\n",
        "    batch_size=batch_size)\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLxT1Ow7Rt9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fits the model on batches with real-time data augmentation:\n",
        "train_datagen.fit(train_images)\n",
        "val_datagen.fit(val_images)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ee638AlnCaWz"
      },
      "source": [
        "*훈련 세트*에서 처음 25개 이미지와 그 아래 클래스 이름을 출력해 보죠. 데이터 포맷이 올바른지 확인하고 네트워크 구성과 훈련할 준비를 마칩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "59veuiEZCaW4"
      },
      "source": [
        "## 모델 구성\n",
        "\n",
        "신경망 모델을 만들려면 모델의 층을 구성한 다음 모델을 컴파일합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_s4MLmc1d5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imported Modules\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Activation, Dense, Flatten, Conv2D, MaxPooling2D, \n",
        "    GlobalAveragePooling2D, AveragePooling2D, BatchNormalization, add)\n",
        "import tensorflow.keras.regularizers as regulizers\n",
        "\n",
        "# Function Definitions\n",
        "\n",
        "def _res_conv(filters, kernel_size=3, padding='same', strides=1, use_relu=True, use_bias=False, name='cbr',\n",
        "              kernel_initializer='he_normal', kernel_regularizer=regulizers.l2(1e-4)):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        conv = Conv2D(\n",
        "            filters=filters, kernel_size=kernel_size, padding=padding, strides=strides, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer, \n",
        "            name=name + '_c')(x)\n",
        "        res = BatchNormalization(axis=-1, name=name + '_bn')(conv)\n",
        "        if use_relu:\n",
        "            res = Activation(\"relu\", name=name + '_r')(res)\n",
        "        return res\n",
        "\n",
        "    return layer_fn\n",
        "\n",
        "\n",
        "def _merge_with_shortcut(kernel_initializer='he_normal', kernel_regularizer=regulizers.l2(1e-4), \n",
        "                         name='block'):\n",
        "\n",
        "    def layer_fn(x, x_residual):\n",
        "        # We check if `x_residual` was scaled down. If so, we scale `x` accordingly with a 1x1 conv:\n",
        "        x_shape = tf.keras.backend.int_shape(x)\n",
        "        x_residual_shape = tf.keras.backend.int_shape(x_residual)\n",
        "        if x_shape == x_residual_shape:\n",
        "            shortcut = x\n",
        "        else:\n",
        "            strides = (\n",
        "                int(round(x_shape[1] / x_residual_shape[1])), # vertical stride\n",
        "                int(round(x_shape[2] / x_residual_shape[2]))  # horizontal stride\n",
        "            )\n",
        "            x_residual_channels = x_residual_shape[3]\n",
        "            shortcut = Conv2D(\n",
        "                filters=x_residual_channels, kernel_size=(1, 1), padding=\"valid\", strides=strides,\n",
        "                kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer,\n",
        "                name=name + '_shortcut_c')(x)\n",
        "\n",
        "        merge = add([shortcut, x_residual])\n",
        "        return merge\n",
        "\n",
        "    return layer_fn\n",
        "\n",
        "\n",
        "def _residual_block_basic(filters, kernel_size=3, strides=1, use_bias=False, name='res_basic',\n",
        "                          kernel_initializer='he_normal', kernel_regularizer=regulizers.l2(1e-4)):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        x_conv1 = _res_conv(\n",
        "            filters=filters, kernel_size=kernel_size, padding='same', strides=strides, \n",
        "            use_relu=True, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer,\n",
        "            name=name + '_cbr_1')(x)\n",
        "        x_residual = _res_conv(\n",
        "            filters=filters, kernel_size=kernel_size, padding='same', strides=1, \n",
        "            use_relu=False, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer,\n",
        "            name=name + '_cbr_2')(x_conv1)\n",
        "        merge = _merge_with_shortcut(kernel_initializer, kernel_regularizer,name=name)(x, x_residual)\n",
        "        merge = Activation('relu')(merge)\n",
        "        return merge\n",
        "\n",
        "    return layer_fn\n",
        "\n",
        "\n",
        "def _residual_block_bottleneck(filters, kernel_size=3, strides=1, use_bias=False, name='res_bottleneck',\n",
        "                               kernel_initializer='he_normal', kernel_regularizer=regulizers.l2(1e-4)):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        x_bottleneck = _res_conv(\n",
        "            filters=filters, kernel_size=1, padding='valid', strides=strides, \n",
        "            use_relu=True, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer,\n",
        "            name=name + '_cbr1')(x)\n",
        "        x_conv = _res_conv(\n",
        "            filters=filters, kernel_size=kernel_size, padding='same', strides=1, \n",
        "            use_relu=True, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer,\n",
        "            name=name + '_cbr2')(x_bottleneck)\n",
        "        x_residual = _res_conv(\n",
        "            filters=filters * 4, kernel_size=1, padding='valid', strides=1, \n",
        "            use_relu=False, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer,\n",
        "            name=name + '_cbr3')(x_conv)\n",
        "        merge = _merge_with_shortcut(kernel_initializer, kernel_regularizer, name=name)(x, x_residual)\n",
        "        merge = Activation('relu')(merge)\n",
        "        return merge\n",
        "\n",
        "    return layer_fn\n",
        "\n",
        "\n",
        "def _residual_macroblock(block_fn, filters, repetitions=3, kernel_size=3, strides_1st_block=1, use_bias=False,\n",
        "                         kernel_initializer='he_normal', kernel_regularizer=regulizers.l2(1e-4),\n",
        "                         name='res_macroblock'):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        for i in range(repetitions):\n",
        "            block_name = \"{}_{}\".format(name, i) \n",
        "            strides = strides_1st_block if i == 0 else 1\n",
        "            x = block_fn(filters=filters, kernel_size=kernel_size, \n",
        "                         strides=strides, use_bias=use_bias,\n",
        "                         kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer,\n",
        "                         name=block_name)(x)\n",
        "        return x\n",
        "\n",
        "    return layer_fn\n",
        "\n",
        "\n",
        "def CResNet(input_shape, num_classes=1000, crop_h=2, crop_w=1, block_fn=_residual_block_basic, repetitions=(2, 2, 2, 2),\n",
        "           use_bias=False, kernel_initializer='he_normal', kernel_regularizer=regulizers.l2(1e-4)):\n",
        "\n",
        "    # Input and 1st layers:\n",
        "    inputs = Input(shape=input_shape)\n",
        "    crop = layers.Cropping2D(cropping=((crop_h, crop_h), (crop_w, crop_w)))(inputs)\n",
        "    conv = _res_conv(\n",
        "        filters=64, kernel_size=7, strides=2, use_relu=True, use_bias=use_bias,\n",
        "        kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(crop)\n",
        "    maxpool = MaxPooling2D(pool_size=3, strides=2, padding='same')(conv)\n",
        "\n",
        "    # Chain of residual blocks:\n",
        "    filters = 64\n",
        "    strides = 2\n",
        "    res_block = maxpool\n",
        "    for i, repet in enumerate(repetitions):\n",
        "        # We do not further reduce the input size for the 1st block (max-pool applied just before):\n",
        "        block_strides = strides if i != 0 else 1\n",
        "        macroblock_name = \"block_{}\".format(i) \n",
        "        res_block = _residual_macroblock(\n",
        "            block_fn=block_fn, repetitions=repet, name=macroblock_name,\n",
        "            filters=filters, strides_1st_block=block_strides, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(res_block)\n",
        "        filters = min(filters * 2, 1024) # we limit to 1024 filters max\n",
        "\n",
        "    # Final layers for prediction:\n",
        "    res_spatial_dim = tf.keras.backend.int_shape(res_block)[1:3]\n",
        "    avg_pool = AveragePooling2D(pool_size=res_spatial_dim, strides=1)(res_block)\n",
        "    flatten = Flatten()(avg_pool)\n",
        "    predictions = Dense(units=num_classes, kernel_initializer=kernel_initializer, \n",
        "                        activation='softmax')(flatten)\n",
        "\n",
        "    # Model:\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "\n",
        "def CResNet18(input_shape, num_classes=1000, crop_h=2, crop_w=1, use_bias=True,\n",
        "             kernel_initializer='he_normal', kernel_regularizer=None):\n",
        "    return CResNet(input_shape, num_classes, crop_h, crop_w, block_fn=_residual_block_basic, repetitions=(2, 2, 2, 2),\n",
        "                  use_bias=use_bias, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)\n",
        "\n",
        "\n",
        "def CResNet34(input_shape, num_classes=1000, crop_h=2, crop_w=1, use_bias=True,\n",
        "             kernel_initializer='he_normal', kernel_regularizer=None):\n",
        "    return CResNet(input_shape, num_classes, crop_h, crop_w, block_fn=_residual_block_basic, repetitions=(3, 4, 6, 3),\n",
        "                  use_bias=use_bias, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)\n",
        "\n",
        "\n",
        "def CResNet50(input_shape, num_classes=1000, crop_h=2, crop_w=1, use_bias=True,\n",
        "             kernel_initializer='he_normal', kernel_regularizer=None):\n",
        "    # Note: CResNet50 is similar to CResNet34,\n",
        "    # with the basic blocks replaced by bottleneck ones (3 conv layers each instead of 2)\n",
        "    return CResNet(input_shape, num_classes, crop_h, crop_w, block_fn=_residual_block_bottleneck, repetitions=(3, 4, 6, 3),\n",
        "                  use_bias=use_bias, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)\n",
        "\n",
        "\n",
        "def CResNet101(input_shape, num_classes=1000, crop_h=2, crop_w=1, use_bias=True,\n",
        "             kernel_initializer='he_normal', kernel_regularizer=None):\n",
        "    return CResNet(input_shape, num_classes, crop_h, crop_w, block_fn=_residual_block_bottleneck, repetitions=(3, 4, 23, 3),\n",
        "                  use_bias=use_bias, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)\n",
        "\n",
        "\n",
        "def CResNet152(input_shape, num_classes=1000, use_bias=True,\n",
        "             kernel_initializer='he_normal', kernel_regularizer=None, crop_h=2, crop_w=1):\n",
        "    return CResNet(input_shape, num_classes, crop_h, crop_w, block_fn=_residual_block_bottleneck, repetitions=(3, 8, 36, 3),\n",
        "                  use_bias=use_bias, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK-Gf8BZpotC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw3oLAHhN-vF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23f99a68-5da6-4855-dee0-cd4daa48ec5e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d (Cropping2D)         (None, 24, 26, 1)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "cbr_c (Conv2D)                  (None, 12, 13, 64)   3200        cropping2d[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cbr_bn (BatchNormalization)     (None, 12, 13, 64)   256         cbr_c[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "cbr_r (Activation)              (None, 12, 13, 64)   0           cbr_bn[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 6, 7, 64)     0           cbr_r[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block_0_0_cbr_1_c (Conv2D)      (None, 6, 7, 64)     36928       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block_0_0_cbr_1_bn (BatchNormal (None, 6, 7, 64)     256         block_0_0_cbr_1_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_0_0_cbr_1_r (Activation)  (None, 6, 7, 64)     0           block_0_0_cbr_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_0_0_cbr_2_c (Conv2D)      (None, 6, 7, 64)     36928       block_0_0_cbr_1_r[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_0_0_cbr_2_bn (BatchNormal (None, 6, 7, 64)     256         block_0_0_cbr_2_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 6, 7, 64)     0           max_pooling2d[0][0]              \n",
            "                                                                 block_0_0_cbr_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 6, 7, 64)     0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block_0_1_cbr_1_c (Conv2D)      (None, 6, 7, 64)     36928       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block_0_1_cbr_1_bn (BatchNormal (None, 6, 7, 64)     256         block_0_1_cbr_1_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_0_1_cbr_1_r (Activation)  (None, 6, 7, 64)     0           block_0_1_cbr_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_0_1_cbr_2_c (Conv2D)      (None, 6, 7, 64)     36928       block_0_1_cbr_1_r[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_0_1_cbr_2_bn (BatchNormal (None, 6, 7, 64)     256         block_0_1_cbr_2_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 6, 7, 64)     0           activation[0][0]                 \n",
            "                                                                 block_0_1_cbr_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 6, 7, 64)     0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block_1_0_cbr_1_c (Conv2D)      (None, 3, 4, 128)    73856       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_1_0_cbr_1_bn (BatchNormal (None, 3, 4, 128)    512         block_1_0_cbr_1_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_0_cbr_1_r (Activation)  (None, 3, 4, 128)    0           block_1_0_cbr_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_1_0_cbr_2_c (Conv2D)      (None, 3, 4, 128)    147584      block_1_0_cbr_1_r[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_0_shortcut_c (Conv2D)   (None, 3, 4, 128)    8320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_1_0_cbr_2_bn (BatchNormal (None, 3, 4, 128)    512         block_1_0_cbr_2_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 3, 4, 128)    0           block_1_0_shortcut_c[0][0]       \n",
            "                                                                 block_1_0_cbr_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 3, 4, 128)    0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block_1_1_cbr_1_c (Conv2D)      (None, 3, 4, 128)    147584      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_1_1_cbr_1_bn (BatchNormal (None, 3, 4, 128)    512         block_1_1_cbr_1_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_1_cbr_1_r (Activation)  (None, 3, 4, 128)    0           block_1_1_cbr_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_1_1_cbr_2_c (Conv2D)      (None, 3, 4, 128)    147584      block_1_1_cbr_1_r[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_1_cbr_2_bn (BatchNormal (None, 3, 4, 128)    512         block_1_1_cbr_2_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 3, 4, 128)    0           activation_2[0][0]               \n",
            "                                                                 block_1_1_cbr_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 3, 4, 128)    0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block_2_0_cbr_1_c (Conv2D)      (None, 2, 2, 256)    295168      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_2_0_cbr_1_bn (BatchNormal (None, 2, 2, 256)    1024        block_2_0_cbr_1_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_0_cbr_1_r (Activation)  (None, 2, 2, 256)    0           block_2_0_cbr_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_2_0_cbr_2_c (Conv2D)      (None, 2, 2, 256)    590080      block_2_0_cbr_1_r[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_0_shortcut_c (Conv2D)   (None, 2, 2, 256)    33024       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_2_0_cbr_2_bn (BatchNormal (None, 2, 2, 256)    1024        block_2_0_cbr_2_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 2, 2, 256)    0           block_2_0_shortcut_c[0][0]       \n",
            "                                                                 block_2_0_cbr_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 2, 2, 256)    0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block_2_1_cbr_1_c (Conv2D)      (None, 2, 2, 256)    590080      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_2_1_cbr_1_bn (BatchNormal (None, 2, 2, 256)    1024        block_2_1_cbr_1_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_1_cbr_1_r (Activation)  (None, 2, 2, 256)    0           block_2_1_cbr_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_2_1_cbr_2_c (Conv2D)      (None, 2, 2, 256)    590080      block_2_1_cbr_1_r[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_1_cbr_2_bn (BatchNormal (None, 2, 2, 256)    1024        block_2_1_cbr_2_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 2, 2, 256)    0           activation_4[0][0]               \n",
            "                                                                 block_2_1_cbr_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 2, 2, 256)    0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block_3_0_cbr_1_c (Conv2D)      (None, 1, 1, 512)    1180160     activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_3_0_cbr_1_bn (BatchNormal (None, 1, 1, 512)    2048        block_3_0_cbr_1_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_0_cbr_1_r (Activation)  (None, 1, 1, 512)    0           block_3_0_cbr_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_3_0_cbr_2_c (Conv2D)      (None, 1, 1, 512)    2359808     block_3_0_cbr_1_r[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_0_shortcut_c (Conv2D)   (None, 1, 1, 512)    131584      activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_3_0_cbr_2_bn (BatchNormal (None, 1, 1, 512)    2048        block_3_0_cbr_2_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 1, 1, 512)    0           block_3_0_shortcut_c[0][0]       \n",
            "                                                                 block_3_0_cbr_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 1, 1, 512)    0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block_3_1_cbr_1_c (Conv2D)      (None, 1, 1, 512)    2359808     activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_3_1_cbr_1_bn (BatchNormal (None, 1, 1, 512)    2048        block_3_1_cbr_1_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_1_cbr_1_r (Activation)  (None, 1, 1, 512)    0           block_3_1_cbr_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_3_1_cbr_2_c (Conv2D)      (None, 1, 1, 512)    2359808     block_3_1_cbr_1_r[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_1_cbr_2_bn (BatchNormal (None, 1, 1, 512)    2048        block_3_1_cbr_2_c[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 1, 1, 512)    0           activation_6[0][0]               \n",
            "                                                                 block_3_1_cbr_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 1, 1, 512)    0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 512)    0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 512)          0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5130        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 11,186,186\n",
            "Trainable params: 11,178,378\n",
            "Non-trainable params: 7,808\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcZ_JR2d_Hvu",
        "colab_type": "text"
      },
      "source": [
        "### CResNET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gxg1XGm0eOBy"
      },
      "source": [
        "### 층 설정\n",
        "\n",
        "신경망의 기본 구성 요소는 *층*(layer)입니다. 층은 주입된 데이터에서 표현을 추출합니다. 아마도 문제를 해결하는데 더 의미있는 표현이 추출될 것입니다.\n",
        "\n",
        "대부분 딥러닝은 간단한 층을 연결하여 구성됩니다. `tf.keras.layers.Dense`와 같은 층들의 가중치(parameter)는 훈련하는 동안 학습됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gut8A_7rCaW6"
      },
      "source": [
        "이 네트워크의 첫 번째 층인 `tf.keras.layers.Flatten`은 2차원 배열(28 x 28 픽셀)의 이미지 포맷을 28 * 28 = 784 픽셀의 1차원 배열로 변환합니다. 이 층은 이미지에 있는 픽셀의 행을 펼쳐서 일렬로 늘립니다. 이 층에는 학습되는 가중치가 없고 데이터를 변환하기만 합니다.\n",
        "\n",
        "픽셀을 펼친 후에는 두 개의 `tf.keras.layers.Dense` 층이 연속되어 연결됩니다. 이 층을 밀집 연결(densely-connected) 또는 완전 연결(fully-connected) 층이라고 부릅니다. 첫 번째 `Dense` 층은 128개의 노드(또는 뉴런)를 가집니다. 두 번째 (마지막) 층은 10개의 노드의 *소프트맥스*(softmax) 층입니다. 이 층은 10개의 확률을 반환하고 반환된 값의 전체 합은 1입니다. 각 노드는 현재 이미지가 10개 클래스 중 하나에 속할 확률을 출력합니다.\n",
        "\n",
        "### 모델 컴파일\n",
        "\n",
        "모델을 훈련하기 전에 필요한 몇 가지 설정이 모델 *컴파일* 단계에서 추가됩니다:\n",
        "\n",
        "* *손실 함수*(Loss function)-훈련 하는 동안 모델의 오차를 측정합니다. 모델의 학습이 올바른 방향으로 향하도록 이 함수를 최소화해야 합니다.\n",
        "* *옵티마이저*(Optimizer)-데이터와 손실 함수를 바탕으로 모델의 업데이트 방법을 결정합니다.\n",
        "* *지표*(Metrics)-훈련 단계와 테스트 단계를 모니터링하기 위해 사용합니다. 다음 예에서는 올바르게 분류된 이미지의 비율인 *정확도*를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeyQSJRzxBj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # learning_rate_decay\n",
        "# initial_learning_rate = 0.001\n",
        "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate,\n",
        "#     decay_steps=10000,\n",
        "#     decay_rate=0.96,\n",
        "#     staircase=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESzrsY7kwu9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model compile\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "#     learning_rate=lr_schedule\n",
        "# ), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSzZiKghbpGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## for reduceLRonPlateau on callback\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qKF6uW-BCaW-"
      },
      "source": [
        "## 모델 훈련\n",
        "\n",
        "신경망 모델을 훈련하는 단계는 다음과 같습니다:\n",
        "\n",
        "1. 훈련 데이터를 모델에 주입합니다-이 예에서는 `train_images`와 `train_labels` 배열입니다.\n",
        "2. 모델이 이미지와 레이블을 매핑하는 방법을 배웁니다.\n",
        "3. 테스트 세트에 대한 모델의 예측을 만듭니다-이 예에서는 `test_images` 배열입니다. 이 예측이 `test_labels` 배열의 레이블과 맞는지 확인합니다.\n",
        "\n",
        "훈련을 시작하기 위해 `model.fit` 메서드를 호출하면 모델이 훈련 데이터를 학습합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPfcGoVXzAfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#callbacks\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hACJCOBwHB3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4881b856-4659-41d3-c05d-fef9ad95c82d"
      },
      "source": [
        "#model fit\n",
        "train_len=len(train_images)\n",
        "val_len=len(val_images)\n",
        "\n",
        "#batch_size is designated in data_preprocessing\n",
        "\n",
        "max_epochs = 50\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps= val_len/batch_size,\n",
        "    epochs= max_epochs, callbacks=[early_stop, reduce_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2000/2000 [==============================] - 41s 21ms/step - loss: 0.7440 - accuracy: 0.7229 - val_loss: 0.5879 - val_accuracy: 0.7830\n",
            "Epoch 2/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.5528 - accuracy: 0.7910 - val_loss: 0.5114 - val_accuracy: 0.7960\n",
            "Epoch 3/50\n",
            "2000/2000 [==============================] - 41s 20ms/step - loss: 0.5077 - accuracy: 0.8066 - val_loss: 0.4797 - val_accuracy: 0.8215\n",
            "Epoch 4/50\n",
            "2000/2000 [==============================] - 41s 20ms/step - loss: 0.4675 - accuracy: 0.8234 - val_loss: 0.4484 - val_accuracy: 0.8292\n",
            "Epoch 5/50\n",
            "2000/2000 [==============================] - 41s 20ms/step - loss: 0.4460 - accuracy: 0.8312 - val_loss: 0.4301 - val_accuracy: 0.8285\n",
            "Epoch 6/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.4226 - accuracy: 0.8398 - val_loss: 0.4066 - val_accuracy: 0.8417\n",
            "Epoch 7/50\n",
            "2000/2000 [==============================] - 41s 20ms/step - loss: 0.4040 - accuracy: 0.8466 - val_loss: 0.4243 - val_accuracy: 0.8423\n",
            "Epoch 8/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3913 - accuracy: 0.8518 - val_loss: 0.3627 - val_accuracy: 0.8565\n",
            "Epoch 9/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3791 - accuracy: 0.8571 - val_loss: 0.3607 - val_accuracy: 0.8689\n",
            "Epoch 10/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3671 - accuracy: 0.8619 - val_loss: 0.3659 - val_accuracy: 0.8578\n",
            "Epoch 11/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3560 - accuracy: 0.8654 - val_loss: 0.3792 - val_accuracy: 0.8548\n",
            "Epoch 12/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3510 - accuracy: 0.8665 - val_loss: 0.3209 - val_accuracy: 0.8784\n",
            "Epoch 13/50\n",
            "2000/2000 [==============================] - 41s 20ms/step - loss: 0.3462 - accuracy: 0.8697 - val_loss: 0.3138 - val_accuracy: 0.8818\n",
            "Epoch 14/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3361 - accuracy: 0.8729 - val_loss: 0.3034 - val_accuracy: 0.8829\n",
            "Epoch 15/50\n",
            "2000/2000 [==============================] - 41s 20ms/step - loss: 0.3324 - accuracy: 0.8744 - val_loss: 0.2799 - val_accuracy: 0.8916\n",
            "Epoch 16/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3223 - accuracy: 0.8774 - val_loss: 0.2836 - val_accuracy: 0.8913\n",
            "Epoch 17/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3172 - accuracy: 0.8804 - val_loss: 0.2754 - val_accuracy: 0.8951\n",
            "Epoch 18/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3129 - accuracy: 0.8812 - val_loss: 0.3044 - val_accuracy: 0.8844\n",
            "Epoch 19/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3113 - accuracy: 0.8830 - val_loss: 0.3141 - val_accuracy: 0.8775\n",
            "Epoch 20/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3045 - accuracy: 0.8846 - val_loss: 0.2697 - val_accuracy: 0.8956\n",
            "Epoch 21/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.3019 - accuracy: 0.8855 - val_loss: 0.2807 - val_accuracy: 0.8930\n",
            "Epoch 22/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2963 - accuracy: 0.8874 - val_loss: 0.2777 - val_accuracy: 0.8908\n",
            "Epoch 23/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2945 - accuracy: 0.8888 - val_loss: 0.2854 - val_accuracy: 0.8909\n",
            "Epoch 24/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2619 - accuracy: 0.9007 - val_loss: 0.2279 - val_accuracy: 0.9148\n",
            "Epoch 25/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2496 - accuracy: 0.9039 - val_loss: 0.2224 - val_accuracy: 0.9119\n",
            "Epoch 26/50\n",
            "2000/2000 [==============================] - 41s 20ms/step - loss: 0.2431 - accuracy: 0.9081 - val_loss: 0.2170 - val_accuracy: 0.9174\n",
            "Epoch 27/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2413 - accuracy: 0.9082 - val_loss: 0.2225 - val_accuracy: 0.9149\n",
            "Epoch 28/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2363 - accuracy: 0.9096 - val_loss: 0.2177 - val_accuracy: 0.9179\n",
            "Epoch 29/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2317 - accuracy: 0.9116 - val_loss: 0.2040 - val_accuracy: 0.9222\n",
            "Epoch 30/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2317 - accuracy: 0.9103 - val_loss: 0.2167 - val_accuracy: 0.9127\n",
            "Epoch 31/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2265 - accuracy: 0.9133 - val_loss: 0.2098 - val_accuracy: 0.9202\n",
            "Epoch 32/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2263 - accuracy: 0.9134 - val_loss: 0.2097 - val_accuracy: 0.9188\n",
            "Epoch 33/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2226 - accuracy: 0.9146 - val_loss: 0.2170 - val_accuracy: 0.9161\n",
            "Epoch 34/50\n",
            "2000/2000 [==============================] - 40s 20ms/step - loss: 0.2207 - accuracy: 0.9153 - val_loss: 0.2041 - val_accuracy: 0.9232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f249203e198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W3ZVOhugCaXA"
      },
      "source": [
        "모델이 훈련되면서 손실과 정확도 지표가 출력됩니다. 이 모델은 훈련 세트에서 약 0.88(88%) 정도의 정확도를 달성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oEw4bZgGCaXB"
      },
      "source": [
        "## 정확도 평가\n",
        "\n",
        "그다음 테스트 세트에서 모델의 성능을 비교합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJumvAzeWrQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4647e1c-3bc4-4bc3-8262-0a96fe961001"
      },
      "source": [
        "test_images = test_images/255\n",
        "test_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VflXLEeECaXC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1917620f-be37-4c09-cc04-593e37f0b2e4"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\n테스트 정확도:', test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 - 2s - loss: 0.2655 - accuracy: 0.9075\n",
            "\n",
            "테스트 정확도: 0.9075000286102295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yWfgsmVXCaXG"
      },
      "source": [
        "테스트 세트의 정확도가 훈련 세트의 정확도보다 조금 낮습니다. 훈련 세트의 정확도와 테스트 세트의 정확도 사이의 차이는 *과대적합*(overfitting) 때문입니다. 과대적합은 머신러닝 모델이 훈련 데이터보다 새로운 데이터에서 성능이 낮아지는 현상을 말합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xsoS7CPDCaXH"
      },
      "source": [
        "## 모델별 Test 정확도\n",
        "\n",
        "훈련된 모델을 사용하여 이미지에 대한 예측을 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0sbZjfSXt4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batchsize = 50 (steps/epoch=1000), lr decay: steps 10000,  Earlystopping: patience 5\n",
        "  # ResNet18: 89.0\n",
        "  # ResNet34\n",
        "  # ResNet50\n",
        "  # ResNet101\n",
        "\n",
        "#batchsize = 50 (steps/epoch=1000), lr decay: steps 50000,  Earlystopping: patience 7 고려?\n",
        "  # ResNet18: 89?\n",
        "  # ResNet34\n",
        "  # ResNet50\n",
        "  # ResNet101\n",
        "#batchsize = 50 (steps/epoch=1000), lr decay: steps 50000,  Earlystopping: pat=5-> ReduceLRonPlateau: pat=3\n",
        "  # ResNet18: 89.7\n",
        "  # ResNet34: 90.57\n",
        "  # ResNet50: 88.86\n",
        "  # ResNet101\n",
        "  ## CResNet18: 90.10\n",
        "#batchsize = 50 (steps/epoch=1000), lr decay: steps 50000,  Earlystopping: pat=5-> ReduceLRonPlateau: pat=3,\n",
        "#cutout regularizaion\n",
        "  # ResNet18:\n",
        "  # ResNet34: \n",
        "  # ResNet50: \n",
        "  # ResNet101\n",
        "  ## CResNet18: 90.7"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}